{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "about-spine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cameras\n",
    "import data_utils\n",
    "import linear_model\n",
    "import procrustes\n",
    "import viz\n",
    "import glob\n",
    "import cdflib\n",
    "sys.argv = sys.argv[:1]\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import data_process as data_process\n",
    "\n",
    "import json \n",
    "from model import LinearModel, OptunaModel, weight_init\n",
    "import torch.nn as nn\n",
    "import utils as utils\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), \"progress\"))\n",
    "\n",
    "from progress.bar import Bar as Bar\n",
    "\n",
    "# Load Human3.6M Skeleton\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pickle\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"action\",\"All\", \"The action to train on. 'All' means all the actions\")\n",
    "\n",
    "# Directories\n",
    "tf.app.flags.DEFINE_string(\"cameras_path\",\"../data/h36m/metadata.xml\", \"File with h36m metadata, including cameras\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-telephone",
   "metadata": {},
   "source": [
    "# Initiate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occasional-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECT_IDS = [1,5,6,7,8,9,11]\n",
    "this_file = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "#Load metadata.xml camera\n",
    "rcams = cameras.load_cameras(os.path.join(this_file, FLAGS.cameras_path), SUBJECT_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "detailed-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Joints in H3.6M -- data has 32 joints, but only 17 that move; these are the indices.\n",
    "H36M_NAMES = ['']*32\n",
    "H36M_NAMES[0]  = 'Hip'\n",
    "H36M_NAMES[1]  = 'RHip'\n",
    "H36M_NAMES[2]  = 'RKnee'\n",
    "H36M_NAMES[3]  = 'RFoot'\n",
    "H36M_NAMES[6]  = 'LHip'\n",
    "H36M_NAMES[7]  = 'LKnee'\n",
    "H36M_NAMES[8]  = 'LFoot'\n",
    "H36M_NAMES[12] = 'Spine'\n",
    "H36M_NAMES[13] = 'Thorax'\n",
    "H36M_NAMES[14] = 'Neck/Nose'\n",
    "H36M_NAMES[15] = 'Head'\n",
    "H36M_NAMES[17] = 'LShoulder'\n",
    "H36M_NAMES[18] = 'LElbow'\n",
    "H36M_NAMES[19] = 'LWrist'\n",
    "H36M_NAMES[25] = 'RShoulder'\n",
    "H36M_NAMES[26] = 'RElbow'\n",
    "H36M_NAMES[27] = 'RWrist'\n",
    "\n",
    "index_alphapose={\n",
    "    # Use 17 skeleton point\n",
    "    \"Nose\": 0,\n",
    "    \"RShoulder\": 6,\n",
    "    \"RElbow\": 8,\n",
    "    \"RWrist\": 10,\n",
    "    \"LShoulder\": 5,\n",
    "    \"LElbow\": 7,\n",
    "    \"LWrist\": 9,\n",
    "    \"RHip\": 12,\n",
    "    \"RKnee\": 14,\n",
    "    \"RAnkle\": 16,\n",
    "    \"LHip\": 11,\n",
    "    \"LKnee\": 13,\n",
    "    \"LAnkle\": 15,\n",
    "    \"REye\": 2,\n",
    "    \"LEye\": 1,\n",
    "    \"REar\": 4,\n",
    "    \"LEar\": 3\n",
    "}\n",
    "\n",
    "index_mapping={\n",
    "# Alpha Pose to Human 3.6M\n",
    "\"Hip\": [20, 0],\n",
    "\"RHip\": [12,1],\n",
    "\"RKnee\": [14,2],\n",
    "\"RFoot\": [16,3],\n",
    "\"LHip\": [11,6],\n",
    "\"LKnee\": [13,7],\n",
    "\"LFoot\": [15,8],\n",
    "\"Spine\": [19,12],\n",
    "\"Thorax\": [18,13],\n",
    "# \"Nose\": [14,0],\n",
    "\"Head\": [17,15],\n",
    "\"LShoulder\": [5,17],\n",
    "\"LElbow\": [7,18],\n",
    "\"LWrist\": [9,19],\n",
    "\"RShoulder\": [6,25],\n",
    "\"RElbow\": [8,26],\n",
    "\"RWrist\": [10,27]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "index_mapping_nose={\n",
    "# Alpha Pose to Human 3.6M\n",
    "\"Hip\": [20, 0],\n",
    "\"RHip\": [12,1],\n",
    "\"RKnee\": [14,2],\n",
    "\"RFoot\": [16,3],\n",
    "\"LHip\": [11,6],\n",
    "\"LKnee\": [13,7],\n",
    "\"LFoot\": [15,8],\n",
    "\"Spine\": [19,12],\n",
    "\"Thorax\": [18,13],\n",
    "\"Nose\": [0,14],\n",
    "\"Head\": [17,15],\n",
    "\"LShoulder\": [5,17],\n",
    "\"LElbow\": [7,18],\n",
    "\"LWrist\": [9,19],\n",
    "\"RShoulder\": [6,25],\n",
    "\"RElbow\": [8,26],\n",
    "\"RWrist\": [10,27]\n",
    "}\n",
    "\n",
    "\n",
    "def data_converter(data):\n",
    "    data=data['keypoints']\n",
    "    keypoints=[]\n",
    "    kp_score=[]\n",
    "    for a in range (0,len(data)):\n",
    "        score=[]\n",
    "        if ((a+3)%3==0):\n",
    "            keypoints.append(data[a])\n",
    "            keypoints.append(data[a+1])\n",
    "        elif((a+1)%3==0):\n",
    "            score=data[a]\n",
    "            kp_score.append(score)\n",
    "\n",
    "    return keypoints\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eleven-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def count_head(alpha_pose):\n",
    "    x = (alpha_pose[index_alphapose['LEar']*2]+alpha_pose[index_alphapose['REar']*2])/2\n",
    "    y = (alpha_pose[index_alphapose['LEar']*2+1]+alpha_pose[index_alphapose['REar']*2+1])/2\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def count_thorax(alpha_pose):\n",
    "    x = (alpha_pose[index_alphapose['LShoulder']*2]+alpha_pose[index_alphapose['RShoulder']*2])/2\n",
    "    y = (alpha_pose[index_alphapose['LShoulder']*2+1]+alpha_pose[index_alphapose['RShoulder']*2+1])/2\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def count_spine(alpha_pose):\n",
    "    hip_x,hip_y=count_hip(alpha_pose)\n",
    "    thorax_x,thorax_y=count_thorax(alpha_pose)\n",
    "    x = (hip_x+thorax_x)/2\n",
    "    y = (hip_y+thorax_y)/2\n",
    "    return x,y\n",
    "\n",
    "def count_hip(alpha_pose):\n",
    "    x = (alpha_pose[index_alphapose['LHip']*2]+alpha_pose[index_alphapose['RHip']*2])/2\n",
    "    y = (alpha_pose[index_alphapose['LHip']*2+1]+alpha_pose[index_alphapose['RHip']*2+1])/2\n",
    "    return x,y\n",
    "\n",
    "def add_features(alpha_pose):\n",
    "    #Count Head\n",
    "    head_x,head_y=count_head(alpha_pose)\n",
    "    alpha_pose=np.append(alpha_pose,(head_x,head_y))\n",
    "    \n",
    "    #Count Thorax\n",
    "    thorax_x,thorax_y=count_thorax(alpha_pose)\n",
    "    alpha_pose=np.append(alpha_pose,(thorax_x,thorax_y))\n",
    " \n",
    "    \n",
    "    #Count Spine\n",
    "    spine_x,spine_y=count_spine(alpha_pose)\n",
    "    alpha_pose=np.append(alpha_pose,(spine_x,spine_y))\n",
    "    \n",
    "    #Count Hip\n",
    "    hip_x,hip_y=count_hip(alpha_pose)\n",
    "    alpha_pose=np.append(alpha_pose,(hip_x,hip_y))\n",
    "    \n",
    "    return alpha_pose\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "perceived-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is not includding nose for 2D to 3D\n",
    "def map_alpha_to_human(alpha_pose):\n",
    "    alpha_pose=add_features(alpha_pose)\n",
    "    temp_list = [None] * 64\n",
    "    for a,b in index_mapping.items():\n",
    "        temp_list[b[1]*2]=alpha_pose[b[0]*2]\n",
    "        temp_list[b[1]*2+1]=alpha_pose[b[0]*2+1]\n",
    "    human36m=np.asarray(temp_list)\n",
    "    return human36m\n",
    "\n",
    "#This function is includding nose for classification\n",
    "def map_alpha_to_human_classification(alpha_pose):\n",
    "    alpha_pose=add_features(alpha_pose)\n",
    "    temp_list = [None] * 64\n",
    "    for a,b in index_mapping_nose.items():\n",
    "        temp_list[b[1]*2]=alpha_pose[b[0]*2]\n",
    "        temp_list[b[1]*2+1]=alpha_pose[b[0]*2+1]\n",
    "    human36m=np.asarray(temp_list)\n",
    "    return human36m\n",
    "\n",
    "\n",
    "def map_alpha_to_human_classification_json(path):\n",
    "    # Opening JSON file \n",
    "    f = open(path) \n",
    "    converted=[]\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "\n",
    "    for dat in data:\n",
    "        convert=np.asarray(data_converter(dat))\n",
    "        human36m_alpha_example=map_alpha_to_human_classification(convert)\n",
    "        human36m_alpha_example=human36m_alpha_example.astype('float')\n",
    "        converted.append(human36m_alpha_example)\n",
    "\n",
    "    converted=np.asarray(converted) \n",
    "        \n",
    "    # Closing file \n",
    "    f.close() \n",
    "    \n",
    "    return converted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-mouth",
   "metadata": {},
   "source": [
    "# Do 3D Prediction from Custom Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-spectacular",
   "metadata": {},
   "source": [
    "## Using Created Statistic Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interested-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize=True\n",
    "actions = data_utils.define_actions( FLAGS.action )\n",
    "# Human3.6m IDs for training and testing\n",
    "TRAIN_SUBJECTS = [1,5,6,7,8]\n",
    "TEST_SUBJECTS  = [9,11]\n",
    "\n",
    "\n",
    "stat_3D = torch.load('../data/stat_3d.pth.tar')\n",
    "stat_2D = torch.load('../data/stat_2d.pth.tar')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unique-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = stat_2D['mean'],stat_2D['std'],stat_2D['dim_ignore'],stat_2D['dim_use']\n",
    "data_mean_3d, data_std_3d, dim_to_ignore_3d, dim_to_use_3d = stat_3D['mean'],stat_3D['std'],stat_3D['dim_ignore'],stat_3D['dim_use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "younger-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the json data\n",
    "#load json data using json,load(f)\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "class Human36M_testing(Dataset):\n",
    "    def __init__(self, skeleton,many=False):\n",
    "        \"\"\"\n",
    "        :param actions: list of actions to use\n",
    "        :param data_path: path to dataset\n",
    "        :param use_hg: use stacked hourglass detections\n",
    "        :param is_train: load train/test dataset\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_inp, self.test_out = [], []\n",
    "\n",
    "        # loading data\n",
    "        # load test data\n",
    "       \n",
    "        if many:\n",
    "            num_f= skeleton.shape\n",
    "            for i in range(num_f[0]):\n",
    "                self.test_inp.append(skeleton[i])\n",
    "        else:\n",
    "            self.test_inp.append(skeleton)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = torch.from_numpy(self.test_inp[index]).float()\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_inp)\n",
    "    \n",
    "def normalize_single_data(data, data_mean, data_std, dim_to_use ):\n",
    "    \"\"\"Normalizes a dictionary of poses\n",
    "\n",
    "    Args\n",
    "    data: dictionary where values are\n",
    "    data_mean: np vector with the mean of the data\n",
    "    data_std: np vector with the standard deviation of the data\n",
    "    dim_to_use: list of dimensions to keep in the data\n",
    "    Returns\n",
    "    data_out: dictionary with same keys as data, but values have been normalized\n",
    "    \"\"\"\n",
    "\n",
    "    data= data[dim_to_use]\n",
    "    mu = data_mean[dim_to_use]\n",
    "    stddev = data_std[dim_to_use]\n",
    "    data_out= np.divide( (data - mu), stddev )\n",
    "\n",
    "    return data_out\n",
    "def create_datatest(data):\n",
    "    converted=[]\n",
    "    for dat in data:\n",
    "        convert=np.asarray(data_converter(dat))\n",
    "        human36m_alpha_example=map_alpha_to_human(convert)\n",
    "        normalized=normalize_single_data(human36m_alpha_example,data_mean_2d,data_std_2d,dim_to_use_2d)\n",
    "        normalized=normalized.astype('float')\n",
    "        converted.append(normalized)\n",
    "\n",
    "    converted=np.asarray(converted) \n",
    "    test_loader = DataLoader(\n",
    "        dataset=Human36M_testing(converted,True),\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "southwest-advice",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acknowledged-gregory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> creating model\n",
      ">>> total params: 10.60M\n",
      ">>> loading ckpt from '../../3d_pose_baseline_pytorch/checkpoint/test/ckpt_best.pth.tar'\n",
      ">>> ckpt loaded (epoch: 29 | err: 40.643307541837174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Project Lab\\THESIS_FALL_DETECTION\\2D to 3D Pose\\3d_pose_baseline_pytorch\\src\\model.py:11: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight)\n",
      "C:\\Users\\hutom\\anaconda3\\envs\\3DPose\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path='../checkpoint/optuna/ckpt_best_1.pth.tar'\n",
    "# create model\n",
    "print(\">>> creating model\")\n",
    "#model = LinearModel()\n",
    "model = OptunaModel()\n",
    "model = model.cuda()\n",
    "model.apply(weight_init)\n",
    "print(\">>> total params: {:.2f}M\".format(sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "criterion = nn.MSELoss(size_average=True).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n",
    "\n",
    "print(\">>> loading ckpt from '{}'\".format('../../3d_pose_baseline_pytorch/checkpoint/test/ckpt_best.pth.tar'))\n",
    "ckpt = torch.load(model_path)\n",
    "start_epoch = ckpt['epoch']\n",
    "err_best = ckpt['err']\n",
    "glob_step = ckpt['step']\n",
    "lr_now = ckpt['lr']\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "optimizer.load_state_dict(ckpt['optimizer'])\n",
    "print(\">>> ckpt loaded (epoch: {} | err: {})\".format(start_epoch, err_best))\n",
    "\n",
    "new_stat_3d={}\n",
    "new_stat_3d['mean']=data_mean_3d\n",
    "new_stat_3d['std']=data_std_3d\n",
    "new_stat_3d['dim_use']=dim_to_use_3d\n",
    "new_stat_3d['dim_ignore']=dim_to_ignore_3d\n",
    "    \n",
    "def test(test_loader, model, criterion, stat_3d, procrustes=False):\n",
    "    losses = utils.AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    all_dist = []\n",
    "    pred_result=[]\n",
    "    start = time.time()\n",
    "    batch_time = 0\n",
    "    bar = Bar('>>>', fill='>', max=len(test_loader))\n",
    "\n",
    "    for i, inps in enumerate(test_loader):\n",
    "        inputs = Variable(inps.cuda())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        # calculate erruracy\n",
    "        print(outputs.shape)\n",
    "        outputs_unnorm = data_process.unNormalizeData(outputs.data.cpu().numpy(), stat_3d['mean'], stat_3d['std'], stat_3d['dim_use'])\n",
    "\n",
    "        # remove dim ignored\n",
    "        dim_use = np.hstack((np.arange(3), stat_3d['dim_use']))\n",
    "\n",
    "        outputs_use = outputs_unnorm[:, dim_use]\n",
    "        pred_result.append(outputs_unnorm)\n",
    "        \n",
    "        # update summary\n",
    "        if (i + 1) % 100 == 0:\n",
    "            batch_time = time.time() - start\n",
    "            start = time.time()\n",
    "\n",
    "        bar.suffix = '({batch}/{size}) | batch: {batchtime:.4}ms | Total: {ttl} | ETA: {eta:} | loss: {loss:.6f}' \\\n",
    "            .format(batch=i + 1,\n",
    "                    size=len(test_loader),\n",
    "                    batchtime=batch_time * 10.0,\n",
    "                    ttl=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg)\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "    return pred_result\n",
    "\n",
    "# Combine prediction from each batch into one prediction\n",
    "def combine_prediction(pred_result_all):\n",
    "    prediction_list = []\n",
    "    for pred in pred_result_all:\n",
    "        for pre in pred:\n",
    "            prediction_list.append(pre)\n",
    "    prediction_list=np.asarray(prediction_list)\n",
    "    return prediction_list\n",
    "\n",
    "\n",
    "def correct_3D(poses3d_input,poses2d_normalized):\n",
    "    _max = 0\n",
    "    _min = 10000\n",
    "    poses3d=np.copy(poses3d_input)\n",
    "    \n",
    "    spine_x = poses2d_normalized[0][24]\n",
    "    spine_y = poses2d_normalized[0][25]\n",
    "            \n",
    "    \n",
    "    for i in range(poses3d.shape[0]):\n",
    "\n",
    "        for j in range(32):\n",
    "\n",
    "            tmp = poses3d[i][j * 3 + 2]\n",
    "            poses3d[i][j * 3 + 2] = poses3d[i][j * 3 + 1]\n",
    "            poses3d[i][j * 3 + 1] = tmp\n",
    "            if poses3d[i][j * 3 + 2] > _max:\n",
    "                _max = poses3d[i][j * 3 + 2]\n",
    "                print(\"_max: \",_max)\n",
    "            if poses3d[i][j * 3 + 2] < _min:\n",
    "                _min = poses3d[i][j * 3 + 2]\n",
    "                print(\"_min: \",_min)\n",
    "\n",
    "    for i in range(poses3d.shape[0]):\n",
    "        for j in range(32):\n",
    "            poses3d[i][j * 3 + 2] = _max - poses3d[i][j * 3 + 2] + _min\n",
    "            poses3d[i][j * 3] += (spine_x - 630)\n",
    "            poses3d[i][j * 3 + 2] += (500 - spine_y)\n",
    "\n",
    "    return poses3d\n",
    "\n",
    "def inferencealphaposeto3D(path,fixing=False,save_npy=False):\n",
    "    # Opening JSON file \n",
    "    f = open(path) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "\n",
    "\n",
    "    # Closing file \n",
    "    f.close() \n",
    "    \n",
    "    #Create Datatest for 2D to 3D Inference\n",
    "    all_test_data=create_datatest(data)\n",
    "    \n",
    "    #Doing Inference\n",
    "    pred_result_all=test(all_test_data, model, criterion, new_stat_3d) #All\n",
    "    \n",
    "    #Combine Prediction Result\n",
    "    prediction_list=combine_prediction(pred_result_all)\n",
    "    \n",
    "    if fixing:\n",
    "        #Fixing for unity\n",
    "        test_2d_normalized = np.asarray(all_test_data.dataset.test_inp) \n",
    "        fixed=correct_3D(prediction_list,test_2d_normalized)\n",
    "    else:\n",
    "        fixed=prediction_list\n",
    "    \n",
    "    if save_npy:\n",
    "        base=os.path.basename(path)\n",
    "        base=os.path.splitext(base)[0]\n",
    "\n",
    "        with open('../inference_result_npy/'+base+'.npy', 'wb') as f:\n",
    "            np.save(f, fixed)\n",
    "    \n",
    "    return fixed\n",
    "\n",
    "\n",
    "\n",
    "def save_to_json(result_3D, input_path, output_path):\n",
    "    f = open(input_path) \n",
    "    dim_use = np.hstack((np.arange(3), dim_to_use_3d))\n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    \n",
    "    for a,b in zip(data,result_3D):\n",
    "        a['keypoints']=b[dim_use].tolist()\n",
    "        a['visualize']=b.tolist()\n",
    "        \n",
    "    with open(output_path, 'w') as fp:\n",
    "        fp.write(json.dumps(data))\n",
    "    \n",
    "def save_to_json_2D(result_2D, input_path, output_path):\n",
    "    f = open(input_path) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    \n",
    "    for a,b in zip(data,result_2D):\n",
    "        a['keypoints']=b[dim_to_use_2d].tolist()\n",
    "        \n",
    "    with open(output_path, 'w') as fp:\n",
    "        fp.write(json.dumps(data))\n",
    "        \n",
    "def save_to_json_original2D(result_2D, input_path, output_path):\n",
    "    f = open(input_path) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    \n",
    "    a=dim_to_use_2d.tolist()\n",
    "    a.insert(18,28)\n",
    "    a.insert(19,29)\n",
    "    dim_to_use_2d_nose=np.asarray(a)\n",
    "\n",
    "    for a,b in zip(data,result_2D):\n",
    "        a['keypoints']=b[dim_to_use_2d_nose].tolist()\n",
    "        \n",
    "    with open(output_path, 'w') as fp:\n",
    "        fp.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d510021",
   "metadata": {},
   "source": [
    "Inference to json to three kinds:\n",
    "1. Original 3D Baseline\n",
    "2. 3D After doing proscrutes to another angle\n",
    "3. 2D Mapping after doing proscrutes to another angle\n",
    "4. Eucledian normalization of 2D Mapping after doing proscrutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c522b",
   "metadata": {},
   "source": [
    "## Convert only 1 Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f59ebd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferencealphaposeto3D_one(alpha2d,input_type=\"array\"):\n",
    "    converted=[]\n",
    "    if (input_type==\"json\"):\n",
    "        convert=np.asarray(data_converter(alpha2d))\n",
    "    else:\n",
    "        convert=alpha2d\n",
    "        \n",
    "    human36m_alpha_example=map_alpha_to_human(convert)\n",
    "    normalized=normalize_single_data(human36m_alpha_example,data_mean_2d,data_std_2d,dim_to_use_2d)\n",
    "    normalized=normalized.astype('float')\n",
    "    converted.append(normalized)\n",
    "\n",
    "    converted=np.asarray(converted) \n",
    "    test_loader = DataLoader(\n",
    "        dataset=Human36M_testing(converted,True),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True)\n",
    "\n",
    "    #Doing Inference\n",
    "    pred_result_all=test(test_loader, model, criterion, new_stat_3d) #All\n",
    "    dim_use = np.hstack((np.arange(3), dim_to_use_3d))\n",
    "    prediction=pred_result_all[0][0][dim_use]\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "df2d7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../json_test/taipei_nctu.json'\n",
    "f = open(path) \n",
    "data = json.load(f) \n",
    "input_to=np.asarray(data_converter(data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c629a359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([461.12005615, 233.56904602, 468.90252686, 225.78659058,\n",
       "       457.22885132, 221.89537048, 492.24984741, 221.89537048,\n",
       "       453.33761597, 214.1129303 , 488.35861206, 291.93737793,\n",
       "       457.22885132, 256.91638184, 441.66397095, 365.87060547,\n",
       "       418.31661987, 295.82858276, 406.64294434, 346.41448975,\n",
       "       387.18682861, 256.91638184, 437.7727356 , 424.23892212,\n",
       "       414.42538452, 393.10916138, 356.05706787, 490.38970947,\n",
       "       348.27462769, 451.47747803, 391.07806396, 572.10540771,\n",
       "       406.64294434, 533.19317627])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input is array of alpha pose with shape (34,)\n",
    "input_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7cb6efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48])\n"
     ]
    }
   ],
   "source": [
    "# How to use function\n",
    "output_3d=inferencealphaposeto3D_one(input_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5e34c510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,    0.        ,    0.        ,  -49.77304402,\n",
       "        -62.32614906,  117.59067363, -307.91601411,  154.24731682,\n",
       "        155.93615246,  -86.12097786,  444.00297313,  316.93509749,\n",
       "         49.52845513,   62.3380657 , -117.10649166, -288.50437127,\n",
       "        291.49161871, -100.33995767, -150.07396629,  600.10424475,\n",
       "        149.51463976,   83.92732442, -227.67448242,  -77.48904473,\n",
       "        154.72347192, -444.80282908, -175.7982368 ,  108.13523618,\n",
       "       -505.1674914 , -222.23970959,  149.40790704, -618.20086844,\n",
       "       -218.43103906,  195.67075695, -353.37778086, -290.7907501 ,\n",
       "         47.95660251,  -98.49515018, -363.40665736,  -86.22504272,\n",
       "       -167.17680604, -307.83899727,  104.26079803, -516.25417939,\n",
       "        -30.41069205,  -47.18346838, -395.09823463,  136.2746552 ,\n",
       "       -156.87321675, -514.84034373,  -25.39123287])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output is array of 3D Pose baseline with shape (51,)\n",
    "output_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd450b29",
   "metadata": {},
   "source": [
    "## Original 3D Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "foreign-remains",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([887, 48])\n",
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path='../json_test/taipei_nctu.json'\n",
    "base= os.path.splitext(os.path.basename(path))[0]\n",
    "inference_result=inferencealphaposeto3D(path,fixing=False,save_npy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5caa721",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base+'_3D_Original.json'\n",
    "save_to_json(inference_result, path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc8874",
   "metadata": {},
   "source": [
    "## Original 2D Human3.6m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1924b1a",
   "metadata": {},
   "source": [
    "This will save alpha pose in human3.6m format that have nose inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15d28fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base+'_2D_Original.json'\n",
    "mapping_result=map_alpha_to_human_classification_json(path)\n",
    "save_to_json_original2D(mapping_result, path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843e5f3",
   "metadata": {},
   "source": [
    "## 3D After doing proscrutes to another angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28f4f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_transformation_3D(skeleton_a,skeleton_b,fullskeleton):\n",
    "    \n",
    "    # remove dim ignored\n",
    "    dim_use = np.hstack((np.arange(3), new_stat_3d['dim_use']))\n",
    "\n",
    "    gt = skeleton_a[dim_use]\n",
    "    out = skeleton_b[dim_use]\n",
    "    gt = gt.reshape(-1, 3)\n",
    "    out = out.reshape(-1, 3)\n",
    "    _, Z, T, b, c = get_transformation(gt, out, True)\n",
    "    \n",
    "    skeleton3D=[]\n",
    "    for i,skeleton in enumerate(fullskeleton):\n",
    "        skeleton = skeleton.reshape(-1, 3)\n",
    "        skeleton = (b * skeleton.dot(T)) + c\n",
    "        skeleton3D.append(skeleton)\n",
    "    skeleton3D=np.asarray(skeleton3D)\n",
    "    skeleton3D=skeleton3D.reshape(skeleton3D.shape[0],96)\n",
    "\n",
    "    return  skeleton3D\n",
    "    \n",
    "def find_pair_transformation(path_1,pose_class_1,image_id_1,path_2,pose_class_2,image_id_2):\n",
    "    with open(path_1, 'rb') as f:\n",
    "        angle1 = json.load(f)\n",
    "    with open(path_2, 'rb') as f:\n",
    "        angle2 = json.load(f)\n",
    "    \n",
    "    angle1_temp=[]\n",
    "    angle1_all=[]\n",
    "\n",
    "    for a in angle1:\n",
    "        if (a['pose_class']==pose_class_1) and (a['image_id']==image_id_1):\n",
    "            angle1_temp.append(a)\n",
    "    \n",
    "\n",
    "    angle2_before=[]\n",
    "    angle2_all=[]\n",
    "    for a in angle2:\n",
    "        if (a['pose_class']==pose_class_2) and (a['image_id']==image_id_2):\n",
    "            angle2_before.append(a)\n",
    "        angle2_all.append(a['visualize'])\n",
    "    \n",
    "    angle2_transformed_3D=find_transformation_3D(np.asarray(angle1_temp[0]['visualize']),\n",
    "                                             np.asarray(angle2_before[0]['visualize']),\n",
    "                                             np.asarray(angle2_all))\n",
    "    \n",
    "\n",
    "    return angle2_transformed_3D\n",
    "\n",
    "def get_transformation(X, Y, compute_optimal_scale=True):\n",
    "    muX = X.mean(0)\n",
    "    muY = Y.mean(0)\n",
    "\n",
    "    X0 = X - muX\n",
    "    Y0 = Y - muY\n",
    "\n",
    "    ssX = (X0 ** 2.).sum()\n",
    "    ssY = (Y0 ** 2.).sum()\n",
    "\n",
    "    # centred Frobenius norm\n",
    "    normX = np.sqrt(ssX)\n",
    "    normY = np.sqrt(ssY)\n",
    "\n",
    "    # scale to equal (unit) norm\n",
    "    X0 = X0 / normX\n",
    "    Y0 = Y0 / normY\n",
    "\n",
    "    # optimum rotation matrix of Y\n",
    "    A = np.dot(X0.T, Y0)\n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    V = Vt.T\n",
    "    T = np.dot(V, U.T)\n",
    "\n",
    "    # Make sure we have a rotation\n",
    "    detT = np.linalg.det(T)\n",
    "    V[:, -1] *= np.sign(detT)\n",
    "    s[-1] *= np.sign(detT)\n",
    "    T = np.dot(V, U.T)\n",
    "\n",
    "    traceTA = s.sum()\n",
    "\n",
    "    if compute_optimal_scale:  # Compute optimum scaling of Y.\n",
    "        b = traceTA * normX / normY\n",
    "        d = 1 - traceTA ** 2\n",
    "        Z = normX * traceTA * np.dot(Y0, T) + muX\n",
    "    else:  # If no scaling allowed\n",
    "        b = 1\n",
    "        d = 1 + ssY / ssX - 2 * traceTA * normY / normX\n",
    "        Z = normY * np.dot(Y0, T) + muX\n",
    "\n",
    "    c = muX - b * np.dot(muY, T)\n",
    "\n",
    "    return d, Z, T, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb2e0df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference\n",
    "path_1='../inference_result_npy/taipei_nctu_3D_Original.json'\n",
    "pose_class_1=\"Stand_04\"\n",
    "image_id_1= '00040.png'\n",
    "\n",
    "#Want to transform\n",
    "path_2='../inference_result_npy/taoyuan_angle2_3D_Original.json'\n",
    "pose_class_2=\"Standing_02\"\n",
    "image_id_2= '00646.png'\n",
    "\n",
    "transformed=find_pair_transformation(path_1,pose_class_1,image_id_1,path_2,pose_class_2,image_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bebb666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base+'_3D_Proscrutes.json'\n",
    "save_to_json(transformed, path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc056307",
   "metadata": {},
   "source": [
    "## 2D Mapping after doing proscrutes to another angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0436ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def load_cameras(bpath='cameras.h5', subjects=None):\n",
    "    \"\"\"\n",
    "    :param bpath: *.h5\n",
    "    :param subjects:\n",
    "    :return: (dict)\n",
    "    \"\"\"\n",
    "\n",
    "    if subjects is None:\n",
    "        subjects = [1, 5, 6, 7, 8, 9, 11]\n",
    "    rcams = {}\n",
    "\n",
    "    with h5py.File(bpath, 'r') as hf:\n",
    "        for s in subjects:\n",
    "            for c in range(4):  # There are 4 cameras in human3.6m\n",
    "                a = load_camera_params(hf, 'subject%d/camera%d/{0}' % (s, c + 1))\n",
    "                rcams[(s, c + 1)] = a\n",
    "\n",
    "    return rcams\n",
    "\n",
    "\n",
    "def map3dto2dcamera( poses_set, cams, ncams=4 ):\n",
    "    \"\"\"\n",
    "    Project 3d poses using camera parameters\n",
    "\n",
    "    cams: dictionary with camera parameters\n",
    "    ncams: number of cameras per subject\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for cam in range( ncams ):\n",
    "        R, T, f, c, k, p, name = cams[ (11, cam+1) ]\n",
    "        pts2d, _, _, _, _ = project_point_radial( np.reshape(poses_set, [-1, 3]), R, T, f, c, k, p )\n",
    "\n",
    "        pts2d = np.reshape( pts2d, [-1, len(H36M_NAMES)*2] )\n",
    "\n",
    "    return pts2d\n",
    "\n",
    "def project_point_radial(P, R, T, f, c, k, p):\n",
    "    \"\"\"\n",
    "    Args\n",
    "    P: Nx3 points in world coordinates\n",
    "    R: 3x3 Camera rotation matrix\n",
    "    T: 3x1 Camera translation parameters\n",
    "    f: 2x1 (scalar) Camera focal length\n",
    "    c: 2x1 Camera center\n",
    "    k: 3x1 Camera radial distortion coefficients\n",
    "    p: 2x1 Camera tangential distortion coefficients\n",
    "    Returns\n",
    "    Proj: Nx2 points in pixel space\n",
    "    D: 1xN depth of each point in camera space\n",
    "    radial: 1xN radial distortion per point\n",
    "    tan: 1xN tangential distortion per point\n",
    "    r2: 1xN squared radius of the projected points before distortion\n",
    "    \"\"\"\n",
    "\n",
    "    # P is a matrix of 3-dimensional points\n",
    "    assert len(P.shape) == 2\n",
    "    assert P.shape[1] == 3\n",
    "\n",
    "    N = P.shape[0]\n",
    "    X = R.dot(P.T - T)  # rotate and translate\n",
    "    XX = X[:2, :] / X[2, :]  # 2x16\n",
    "    r2 = XX[0, :] ** 2 + XX[1, :] ** 2  # 16,\n",
    "\n",
    "    radial = 1 + np.einsum('ij,ij->j', np.tile(k, (1, N)), np.array([r2, r2 ** 2, r2 ** 3]))  # 16,\n",
    "    tan = p[0] * XX[1, :] + p[1] * XX[0, :]  # 16,\n",
    "\n",
    "    tm = np.outer(np.array([p[1], p[0]]).reshape(-1), r2)  # 2x16\n",
    "\n",
    "    XXX = XX * np.tile(radial + tan, (2, 1)) + tm  # 2x16\n",
    "\n",
    "    Proj = (f * XXX) + c  # 2x16\n",
    "    Proj = Proj.T\n",
    "\n",
    "    D = X[2, ]\n",
    "\n",
    "    return Proj, D, radial, tan, r2\n",
    "\n",
    "\n",
    "def mapto2D(skeletons,rcams):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    skeleton2D=[]\n",
    "    for i,skeleton in enumerate(skeletons):\n",
    "        mapped=map3dto2dcamera(skeleton,rcams,4)\n",
    "        skeleton2D.append(mapped)\n",
    "    skeleton2D=np.asarray(skeleton2D)\n",
    "    skeleton2D=skeleton2D.reshape(skeleton2D.shape[0],64)\n",
    "    return skeleton2D\n",
    "\n",
    "SUBJECT_IDS = [1,5,6,7,8,9,11]\n",
    "this_file = os.path.dirname(os.path.realpath('__file__'))\n",
    "rcams = cameras.load_cameras(os.path.join(this_file, \"../data/h36m/metadata.xml\"), SUBJECT_IDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac2a9147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skeleton2d=mapto2D(transformed,rcams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "690c3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base+'_2D_Proscrutes.json'\n",
    "save_to_json_2D(skeleton2d, path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de7e1c",
   "metadata": {},
   "source": [
    "## Normalize using Eucledian Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9aaa49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_human36m={\n",
    "    # Use 16 skeleton point\n",
    "    \"Pelvis\": 0,\n",
    "    \"Head\": 15,\n",
    "    \"Thorax\":13,\n",
    "    \"RShoulder\": 25,\n",
    "    \"RElbow\": 26,\n",
    "    \"RWrist\": 27,\n",
    "    \"LShoulder\": 17,\n",
    "    \"LElbow\": 18,\n",
    "    \"LWrist\": 19,\n",
    "    \"RHip\": 1,\n",
    "    \"RKnee\": 2,\n",
    "    \"RAnkle\": 3,\n",
    "    \"LHip\": 6,\n",
    "    \"LKnee\": 7,\n",
    "    \"LAnkle\": 8,\n",
    "    \"Spine\" : 12\n",
    "}\n",
    "\n",
    "def euclidean_dist(a, b):\n",
    "    # This function calculates the euclidean distance between 2 point in 2-D coordinates\n",
    "    # if one of two points is (0,0), dist = 0\n",
    "    # a, b: input array with dimension: m, 2\n",
    "    # m: number of samples\n",
    "    # 2: x and y coordinate\n",
    "    try:\n",
    "        if (a.shape[1] == 2 and a.shape == b.shape):\n",
    "            # check if element of a and b is (0,0)\n",
    "            bol_a = (a[:,0] != 0).astype(int)\n",
    "            bol_b = (b[:,0] != 0).astype(int)\n",
    "            dist = np.linalg.norm(a-b, axis=1)\n",
    "            return((dist*bol_a*bol_b).reshape(a.shape[0],1))\n",
    "    except:\n",
    "        print(\"[Error]: Check dimension of input vector\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def norm_human36m(X):\n",
    "    num_sample = X.shape[0]\n",
    "    # Keypoints\n",
    "    Pelvis = X[:,index_human36m['Pelvis']*2:index_human36m['Pelvis']*2+2]\n",
    "    Head = X[:,index_human36m['Head']*2:index_human36m['Head']*2+2]\n",
    "    Thorax = X[:,index_human36m['Thorax']*2:index_human36m['Thorax']*2+2]\n",
    "    RShoulder = X[:,index_human36m['RShoulder']*2:index_human36m['RShoulder']*2+2]\n",
    "    RElbow = X[:,index_human36m['RElbow']*2:index_human36m['RElbow']*2+2]\n",
    "    RWrist = X[:,index_human36m['RWrist']*2:index_human36m['RWrist']*2+2]\n",
    "    LShoulder = X[:,index_human36m['LShoulder']*2:index_human36m['LShoulder']*2+2]\n",
    "    LElbow = X[:,index_human36m['LElbow']*2:index_human36m['LElbow']*2+2]\n",
    "    LWrist = X[:,index_human36m['LWrist']*2:index_human36m['LWrist']*2+2]\n",
    "    RHip = X[:,index_human36m['RHip']*2:index_human36m['RHip']*2+2]\n",
    "    RKnee = X[:,index_human36m['RKnee']*2:index_human36m['RKnee']*2+2]\n",
    "    RAnkle = X[:,index_human36m['RAnkle']*2:index_human36m['RAnkle']*2+2]\n",
    "    LHip = X[:,index_human36m['LHip']*2:index_human36m['LHip']*2+2]\n",
    "    LKnee = X[:,index_human36m['LKnee']*2:index_human36m['LKnee']*2+2]\n",
    "    LAnkle = X[:,index_human36m['LAnkle']*2:index_human36m['LAnkle']*2+2]\n",
    "\n",
    "\n",
    "    # Length of head\n",
    "    length_Head_Thorax = euclidean_dist(Head, Thorax)\n",
    "    length_head      = np.maximum.reduce([length_Head_Thorax])\n",
    "\n",
    "    # Length of torso\n",
    "    length_Thorax_LHip = euclidean_dist(Thorax, LHip)\n",
    "    length_Thorax_RHip = euclidean_dist(Thorax, RHip)\n",
    "    length_torso     = np.maximum(length_Thorax_LHip, length_Thorax_RHip)\n",
    "\n",
    "    # Length of right leg\n",
    "    length_leg_right = euclidean_dist(RHip, RKnee) + euclidean_dist(RKnee, RAnkle)\n",
    "  \n",
    "    # Length of left leg\n",
    "    length_leg_left = euclidean_dist(LHip, LKnee) + euclidean_dist(LKnee, LAnkle)\n",
    "\n",
    "\n",
    "    # Length of leg\n",
    "    length_leg = np.maximum(length_leg_right, length_leg_left)\n",
    "\n",
    "    # Length of body\n",
    "    length_body = length_head + length_torso + length_leg\n",
    "    \n",
    "    # Check all samples have length_body of 0\n",
    "    length_chk = (length_body > 0).astype(int)\n",
    "    \n",
    "    # Check keypoints at origin\n",
    "    keypoints_chk = (X > 0).astype(int)\n",
    "    \n",
    "    chk = length_chk * keypoints_chk\n",
    "    \n",
    "    # Set all length_body of 0 to 1 (to avoid division by 0)\n",
    "    length_body[length_body == 0] = 1\n",
    "    \n",
    "    # The center of gravity\n",
    "    num_pts = (X[:, 0::2] > 0).sum(1).reshape(num_sample,1)\n",
    "\n",
    "    centr_x = X[:, 0::2].sum(1).reshape(num_sample,1) / num_pts\n",
    "    centr_y = X[:, 1::2].sum(1).reshape(num_sample,1) / num_pts\n",
    "\n",
    "    # The  coordinates  are  normalized relative to the length of the body and the center of gravity\n",
    "    X_norm_x = (X[:, 0::2] - centr_x) / length_body\n",
    "    X_norm_y = (X[:, 1::2] - centr_y) / length_body\n",
    "    \n",
    "    # Stack 1st element x and y together\n",
    "    X_norm = np.column_stack((X_norm_x[:,:1], X_norm_y[:,:1]))\n",
    "        \n",
    "    for i in range(1, X.shape[1]//2):\n",
    "        X_norm = np.column_stack((X_norm, X_norm_x[:,i:i+1], X_norm_y[:,i:i+1]))\n",
    "    \n",
    "    # Set all samples have length_body of 0 to origin (0, 0)\n",
    "    X_norm = X_norm * chk\n",
    "    \n",
    "    return X_norm\n",
    "\n",
    "def euclidean_dist_3D(a, b):\n",
    "\n",
    "    try:\n",
    "        if (a.shape[1] == 3 and a.shape == b.shape):\n",
    "            # check if element of a and b is (0,0)\n",
    "            bol_a = (a[:,0] != 0).astype(int)\n",
    "            bol_b = (b[:,0] != 0).astype(int)\n",
    "            dist = np.linalg.norm(a-b, axis=1)\n",
    "            return((dist*bol_a*bol_b).reshape(a.shape[0],1))\n",
    "    except:\n",
    "        print(\"[Error]: Check dimension of input vector\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def norm_human36m_3D(X):\n",
    "    num_sample = X.shape[0]\n",
    "    # Keypoints\n",
    "    Pelvis = X[:,index_human36m['Pelvis']*3:index_human36m['Pelvis']*3+3]\n",
    "    Head = X[:,index_human36m['Head']*3:index_human36m['Head']*3+3]\n",
    "    Thorax = X[:,index_human36m['Thorax']*3:index_human36m['Thorax']*3+3]\n",
    "    RShoulder = X[:,index_human36m['RShoulder']*3:index_human36m['RShoulder']*3+3]\n",
    "    RElbow = X[:,index_human36m['RElbow']*3:index_human36m['RElbow']*3+3]\n",
    "    RWrist = X[:,index_human36m['RWrist']*3:index_human36m['RWrist']*3+3]\n",
    "    LShoulder = X[:,index_human36m['LShoulder']*3:index_human36m['LShoulder']*3+3]\n",
    "    LElbow = X[:,index_human36m['LElbow']*3:index_human36m['LElbow']*3+3]\n",
    "    LWrist = X[:,index_human36m['LWrist']*3:index_human36m['LWrist']*3+3]\n",
    "    RHip = X[:,index_human36m['RHip']*3:index_human36m['RHip']*3+3]\n",
    "    RKnee = X[:,index_human36m['RKnee']*3:index_human36m['RKnee']*3+3]\n",
    "    RAnkle = X[:,index_human36m['RAnkle']*3:index_human36m['RAnkle']*3+3]\n",
    "    LHip = X[:,index_human36m['LHip']*3:index_human36m['LHip']*3+3]\n",
    "    LKnee = X[:,index_human36m['LKnee']*3:index_human36m['LKnee']*3+3]\n",
    "    LAnkle = X[:,index_human36m['LAnkle']*3:index_human36m['LAnkle']*3+3]\n",
    "\n",
    "\n",
    "    # Length of head\n",
    "    length_Head_Thorax = euclidean_dist_3D(Head, Thorax)\n",
    "    length_head      = np.maximum.reduce([length_Head_Thorax])\n",
    "\n",
    "    # Length of torso\n",
    "    length_Thorax_LHip = euclidean_dist_3D(Thorax, LHip)\n",
    "    length_Thorax_RHip = euclidean_dist_3D(Thorax, RHip)\n",
    "    length_torso     = np.maximum(length_Thorax_LHip, length_Thorax_RHip)\n",
    "\n",
    "    # Length of right leg\n",
    "    length_leg_right = euclidean_dist_3D(RHip, RKnee) + euclidean_dist_3D(RKnee, RAnkle)\n",
    "  \n",
    "    # Length of left leg\n",
    "    length_leg_left = euclidean_dist_3D(LHip, LKnee) + euclidean_dist_3D(LKnee, LAnkle)\n",
    "\n",
    "\n",
    "    # Length of leg\n",
    "    length_leg = np.maximum(length_leg_right, length_leg_left)\n",
    "\n",
    "    # Length of body\n",
    "    length_body = length_head + length_torso + length_leg\n",
    "    \n",
    "    # Check all samples have length_body of 0\n",
    "    length_chk = (length_body > 0).astype(int)\n",
    "    \n",
    "    # Check keypoints at origin\n",
    "    keypoints_chk = (X > 0).astype(int)\n",
    "    \n",
    "    chk = length_chk * keypoints_chk\n",
    "    \n",
    "    # Set all length_body of 0 to 1 (to avoid division by 0)\n",
    "    length_body[length_body == 0] = 1\n",
    "    \n",
    "    # The center of gravity\n",
    "    num_pts = np.full((num_sample, 1), 32)\n",
    "    print(num_pts)\n",
    "    centr_x = X[:, 0::3].sum(1).reshape(num_sample,1) / num_pts\n",
    "    centr_y = X[:, 1::3].sum(1).reshape(num_sample,1) / num_pts\n",
    "    centr_z = X[:, 2::3].sum(1).reshape(num_sample,1) / num_pts\n",
    "\n",
    "    # The  coordinates  are  normalized relative to the length of the body and the center of gravity\n",
    "    X_norm_x = (X[:, 0::3] - centr_x) / length_body\n",
    "    X_norm_y = (X[:, 1::3] - centr_y) / length_body\n",
    "    X_norm_z = (X[:, 2::3] - centr_z) / length_body\n",
    "\n",
    "    # Stack 1st element x and y together\n",
    "    X_norm = np.column_stack((X_norm_x[:,:1], X_norm_y[:,:1], X_norm_z[:,:1]))\n",
    "        \n",
    "    for i in range(1, X.shape[1]//3):\n",
    "        X_norm = np.column_stack((X_norm, X_norm_x[:,i:i+1], X_norm_y[:,i:i+1], X_norm_z[:,i:i+1]))\n",
    "    \n",
    "    # Set all samples have length_body of 0 to origin (0, 0)\n",
    "    X_norm = X_norm * chk\n",
    "    \n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c07f5162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "normalized_eucledianhuman36m=norm_human36m(skeleton2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e601d29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7111, 64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_eucledianhuman36m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "455c1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base+'_2D_Proscrutes_Normalized.json'\n",
    "save_to_json_2D(normalized_eucledianhuman36m, path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fe975",
   "metadata": {},
   "source": [
    "# Don't run (just experiment and random code) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c9305b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class=set([])\n",
    "for a in new:\n",
    "    unique_class.add(a['pose_class'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "993bb858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fall_02',\n",
       " 'Fall_03',\n",
       " 'Fall_04',\n",
       " 'Fall_05',\n",
       " 'Fall_06',\n",
       " 'Fall_07',\n",
       " 'Fall_08',\n",
       " 'Fall_09',\n",
       " 'Fall_10',\n",
       " 'Stand_01',\n",
       " 'Stand_02',\n",
       " 'Stand_03',\n",
       " 'Stand_04',\n",
       " 'Stand_05',\n",
       " 'Stand_06',\n",
       " 'Stand_07',\n",
       " 'Stand_08'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a697ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(\"../../../Falling Dataset/Thesis Experiment/data/taoyuan.json\") \n",
    "\n",
    "# returns JSON object as  \n",
    "# a dictionary \n",
    "data = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b736ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "taoyuan_angle1=[]\n",
    "taoyuan_angle2=[]\n",
    "\n",
    "for a in data:\n",
    "    if ((a['pose_class']==\"FallingDown_01\") or (a['pose_class']==\"Standing_01\")):\n",
    "        taoyuan_angle1.append(a)\n",
    "    else:\n",
    "        taoyuan_angle2.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0677c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../Falling Dataset/Thesis Experiment/data/taoyuan_angle1.json\", 'w') as fp:\n",
    "    fp.write(json.dumps(taoyuan_angle1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43866751",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../Falling Dataset/Thesis Experiment/data/taoyuan_angle2.json\", 'w') as fp:\n",
    "    fp.write(json.dumps(taoyuan_angle2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
